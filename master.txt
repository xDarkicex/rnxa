Directory structure:
└── rnxa/
    ├── README.md
    ├── engine.go
    ├── tensor.go
    ├── backends/
    │   └── metal_darwin.go
    └── internal/
        └── metal/
            ├── metal_ops.h
            └── metal_ops.m

================================================
FILE: README.md
================================================
# rnxa



================================================
FILE: engine.go
================================================
package rnxa

import (
	"context"
	"fmt"
)

// ComputeEngine provides hardware-accelerated tensor operations
// Universal interface - works with any Go ML framework
type ComputeEngine interface {
	// Core operations - immediate value for relux
	MatMul(ctx context.Context, A, B *Tensor) (*Tensor, error)
	VectorAdd(ctx context.Context, A, B *Tensor) (*Tensor, error)
	VectorSub(ctx context.Context, A, B *Tensor) (*Tensor, error)
	VectorMul(ctx context.Context, A, B *Tensor) (*Tensor, error) // Element-wise

	// Activation functions - direct relux integration
	ReLU(ctx context.Context, X *Tensor) (*Tensor, error)
	Sigmoid(ctx context.Context, X *Tensor) (*Tensor, error)
	Tanh(ctx context.Context, X *Tensor) (*Tensor, error)
	Softmax(ctx context.Context, X *Tensor) (*Tensor, error)

	// Reduction operations
	Sum(ctx context.Context, X *Tensor, axis int) (*Tensor, error)
	Mean(ctx context.Context, X *Tensor, axis int) (*Tensor, error)

	// Device management
	Device() Device
	Available() bool
	Memory() MemoryInfo
	Close() error
}

// Device represents compute hardware (M2, RTX 4090, etc.)
type Device struct {
	ID       int
	Name     string
	Type     DeviceType
	Memory   uint64 // Available memory in bytes
	Cores    int    // Compute units/cores
	Platform string // "Metal", "CUDA", "OpenCL", "CPU"
}

type DeviceType int

const (
	CPU DeviceType = iota
	GPU
	NPU // Neural Processing Unit (future)
)

type MemoryInfo struct {
	Total     uint64
	Available uint64
	Used      uint64
}

// NewEngine creates the best available compute engine
func NewEngine() (ComputeEngine, error) {
	// Auto-detect best device
	devices := DetectDevices()
	if len(devices) == 0 {
		return newCPUEngine(), nil // Always have CPU fallback
	}

	// Prioritize: Metal (M2) > CUDA > OpenCL > CPU
	for _, device := range devices {
		switch device.Platform {
		case "Metal":
			return newMetalEngine(device)
		case "CUDA":
			return newCUDAEngine(device)
		case "OpenCL":
			return newOpenCLEngine(device)
		}
	}

	return newCPUEngine(), nil
}

// NewEngineWithDevice creates engine for specific device
func NewEngineWithDevice(deviceID int) (ComputeEngine, error) {
	devices := DetectDevices()
	if deviceID >= len(devices) {
		return nil, fmt.Errorf("device %d not found", deviceID)
	}

	device := devices[deviceID]
	switch device.Platform {
	case "Metal":
		return newMetalEngine(device)
	case "CUDA":
		return newCUDAEngine(device)
	default:
		return newCPUEngine(), nil
	}
}



================================================
FILE: tensor.go
================================================
package rnxa

import "fmt"

// Tensor represents n-dimensional arrays with hardware acceleration
type Tensor struct {
	data    []float64 // Host memory (always available)
	gpuData uintptr   // GPU memory handle (platform-specific)
	shape   []int     // Tensor dimensions [batch, height, width, channels]
	stride  []int     // Memory layout stride
	device  Device    // Which device owns this tensor
	dtype   DataType  // float32, float64, int32, etc.
}

type DataType int

const (
	Float32 DataType = iota
	Float64
	Int32
	Int64
)

// Creation functions
func NewTensor(data []float64, shape ...int) *Tensor {
	if len(shape) == 0 {
		shape = []int{len(data)} // 1D vector default
	}

	return &Tensor{
		data:   data,
		shape:  shape,
		stride: computeStride(shape),
		dtype:  Float64,
	}
}

func Zeros(shape ...int) *Tensor {
	size := 1
	for _, dim := range shape {
		size *= dim
	}
	return NewTensor(make([]float64, size), shape...)
}

func Ones(shape ...int) *Tensor {
	size := 1
	for _, dim := range shape {
		size *= dim
	}
	data := make([]float64, size)
	for i := range data {
		data[i] = 1.0
	}
	return NewTensor(data, shape...)
}

// Core tensor operations
func (t *Tensor) Shape() []int    { return t.shape }
func (t *Tensor) Size() int       { return len(t.data) }
func (t *Tensor) Data() []float64 { return t.data }
func (t *Tensor) Device() Device  { return t.device }

func (t *Tensor) Reshape(newShape ...int) *Tensor {
	// Verify compatible size
	oldSize, newSize := 1, 1
	for _, dim := range t.shape {
		oldSize *= dim
	}
	for _, dim := range newShape {
		newSize *= dim
	}

	if oldSize != newSize {
		panic(fmt.Sprintf("cannot reshape tensor of size %d to %d", oldSize, newSize))
	}

	return &Tensor{
		data:   t.data, // Same underlying data
		shape:  newShape,
		stride: computeStride(newShape),
		device: t.device,
		dtype:  t.dtype,
	}
}

// GPU memory management
func (t *Tensor) ToDevice(device Device) *Tensor {
	// Move tensor to specific device (implement per backend)
	// This is where Metal/CUDA-specific code lives
	return t
}

func (t *Tensor) ToHost() *Tensor {
	// Ensure data is in host memory
	return t
}

func computeStride(shape []int) []int {
	stride := make([]int, len(shape))
	if len(shape) == 0 {
		return stride
	}

	stride[len(stride)-1] = 1
	for i := len(stride) - 2; i >= 0; i-- {
		stride[i] = stride[i+1] * shape[i+1]
	}
	return stride
}



================================================
FILE: backends/metal_darwin.go
================================================
//go:build darwin
// +build darwin

package backends

/*
#cgo CFLAGS: -x objective-c
#cgo LDFLAGS: -framework Metal -framework MetalPerformanceShaders -framework Foundation

#include "metal_ops.h"
*/
import "C"
import (
	"context"
	"fmt"
	"unsafe"
)

type metalEngine struct {
	device       Device
	metalDevice  C.MTLDeviceRef
	commandQueue C.MTLCommandQueueRef
}

func newMetalEngine(device Device) (ComputeEngine, error) {
	metalDevice := C.metal_create_device()
	if metalDevice == nil {
		return nil, fmt.Errorf("failed to create Metal device")
	}

	commandQueue := C.metal_create_command_queue(metalDevice)
	if commandQueue == nil {
		C.metal_release_device(metalDevice)
		return nil, fmt.Errorf("failed to create Metal command queue")
	}

	return &metalEngine{
		device:       device,
		metalDevice:  metalDevice,
		commandQueue: commandQueue,
	}, nil
}

func (e *metalEngine) MatMul(ctx context.Context, A, B *Tensor) (*Tensor, error) {
	// Validate inputs
	if len(A.Shape()) != 2 || len(B.Shape()) != 2 {
		return nil, fmt.Errorf("MatMul requires 2D tensors")
	}

	M, K1 := A.Shape()[0], A.Shape()[1]
	K2, N := B.Shape()[0], B.Shape()[1]
	if K1 != K2 {
		return nil, fmt.Errorf("incompatible matrix dimensions: (%d,%d) × (%d,%d)", M, K1, K2, N)
	}

	// Create result tensor
	C_result := Zeros(M, N)

	// Metal Performance Shaders matrix multiplication
	result := C.metal_matrix_multiply(
		e.metalDevice,
		e.commandQueue,
		(*C.float)(unsafe.Pointer(&A.data[0])), C.int(M), C.int(K1),
		(*C.float)(unsafe.Pointer(&B.data[0])), C.int(K2), C.int(N),
		(*C.float)(unsafe.Pointer(&C_result.data[0])),
	)

	if result != 0 {
		return nil, fmt.Errorf("Metal matrix multiplication failed: %d", result)
	}

	return C_result, nil
}

func (e *metalEngine) ReLU(ctx context.Context, X *Tensor) (*Tensor, error) {
	result := Zeros(X.Shape()...)

	success := C.metal_relu(
		e.metalDevice,
		e.commandQueue,
		(*C.float)(unsafe.Pointer(&X.data[0])),
		(*C.float)(unsafe.Pointer(&result.data[0])),
		C.int(X.Size()),
	)

	if success != 0 {
		return nil, fmt.Errorf("Metal ReLU failed")
	}

	return result, nil
}

func (e *metalEngine) Device() Device  { return e.device }
func (e *metalEngine) Available() bool { return e.metalDevice != nil }

func (e *metalEngine) Memory() MemoryInfo {
	return MemoryInfo{
		Total:     uint64(C.metal_get_total_memory(e.metalDevice)),
		Available: uint64(C.metal_get_available_memory(e.metalDevice)),
	}
}

func (e *metalEngine) Close() error {
	if e.commandQueue != nil {
		C.metal_release_command_queue(e.commandQueue)
	}
	if e.metalDevice != nil {
		C.metal_release_device(e.metalDevice)
	}
	return nil
}



================================================
FILE: internal/metal/metal_ops.h
================================================
// Metal Performance Shaders C interface
#ifndef METAL_OPS_H
#define METAL_OPS_H

#import <Metal/Metal.h>
#import <MetalPerformanceShaders/MetalPerformanceShaders.h>

typedef void* MTLDeviceRef;
typedef void* MTLCommandQueueRef;

// Device management
MTLDeviceRef metal_create_device(void);
void metal_release_device(MTLDeviceRef device);
MTLCommandQueueRef metal_create_command_queue(MTLDeviceRef device);
void metal_release_command_queue(MTLCommandQueueRef queue);

// Memory info
size_t metal_get_total_memory(MTLDeviceRef device);
size_t metal_get_available_memory(MTLDeviceRef device);

// Core operations
int metal_matrix_multiply(MTLDeviceRef device, MTLCommandQueueRef queue,
                         const float* A, int M, int K,
                         const float* B, int K2, int N,
                         float* C);

int metal_relu(MTLDeviceRef device, MTLCommandQueueRef queue,
               const float* input, float* output, int size);

int metal_sigmoid(MTLDeviceRef device, MTLCommandQueueRef queue,
                  const float* input, float* output, int size);

#endif



================================================
FILE: internal/metal/metal_ops.m
================================================
#import "metal_ops.h"

MTLDeviceRef metal_create_device(void) {
    id<MTLDevice> device = MTLCreateSystemDefaultDevice();
    return (__bridge_retained MTLDeviceRef)device;
}

void metal_release_device(MTLDeviceRef device) {
    id<MTLDevice> mtlDevice = (__bridge_transfer id<MTLDevice>)device;
    mtlDevice = nil;
}

MTLCommandQueueRef metal_create_command_queue(MTLDeviceRef device) {
    id<MTLDevice> mtlDevice = (__bridge id<MTLDevice>)device;
    id<MTLCommandQueue> queue = [mtlDevice newCommandQueue];
    return (__bridge_retained MTLCommandQueueRef)queue;
}

int metal_matrix_multiply(MTLDeviceRef device, MTLCommandQueueRef queue,
                         const float* A, int M, int K,
                         const float* B, int K2, int N,
                         float* C) {
    @autoreleasepool {
        id<MTLDevice> mtlDevice = (__bridge id<MTLDevice>)device;
        id<MTLCommandQueue> commandQueue = (__bridge id<MTLCommandQueue>)queue;
        
        // Create Metal buffers
        id<MTLBuffer> bufferA = [mtlDevice newBufferWithBytes:A 
                                                       length:M * K * sizeof(float)
                                                      options:MTLResourceStorageModeShared];
        id<MTLBuffer> bufferB = [mtlDevice newBufferWithBytes:B 
                                                       length:K * N * sizeof(float)
                                                      options:MTLResourceStorageModeShared];
        id<MTLBuffer> bufferC = [mtlDevice newBufferWithLength:M * N * sizeof(float)
                                                       options:MTLResourceStorageModeShared];
        
        // Use Metal Performance Shaders for optimal performance
        MPSMatrixDescriptor* descA = [MPSMatrixDescriptor matrixDescriptorWithDimensions:M
                                                                                 columns:K
                                                                                rowBytes:K * sizeof(float)
                                                                                dataType:MPSDataTypeFloat32];
        
        MPSMatrixDescriptor* descB = [MPSMatrixDescriptor matrixDescriptorWithDimensions:K
                                                                                 columns:N
                                                                                rowBytes:N * sizeof(float)
                                                                                dataType:MPSDataTypeFloat32];
        
        MPSMatrixDescriptor* descC = [MPSMatrixDescriptor matrixDescriptorWithDimensions:M
                                                                                 columns:N
                                                                                rowBytes:N * sizeof(float)
                                                                                dataType:MPSDataTypeFloat32];
        
        MPSMatrix* matrixA = [[MPSMatrix alloc] initWithBuffer:bufferA descriptor:descA];
        MPSMatrix* matrixB = [[MPSMatrix alloc] initWithBuffer:bufferB descriptor:descB];
        MPSMatrix* matrixC = [[MPSMatrix alloc] initWithBuffer:bufferC descriptor:descC];
        
        // Perform matrix multiplication: C = A × B
        MPSMatrixMultiplication* matMul = [[MPSMatrixMultiplication alloc] initWithDevice:mtlDevice
                                                                           transposeLeft:NO
                                                                          transposeRight:NO
                                                                              resultRows:M
                                                                           resultColumns:N
                                                                            interiorColumns:K
                                                                                     alpha:1.0
                                                                                      beta:0.0];
        
        id<MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];
        [matMul encodeToCommandBuffer:commandBuffer
                           leftMatrix:matrixA
                          rightMatrix:matrixB
                         resultMatrix:matrixC];
        
        [commandBuffer commit];
        [commandBuffer waitUntilCompleted];
        
        // Copy result back
        memcpy(C, [bufferC contents], M * N * sizeof(float));
        
        return 0; // Success
    }
}

int metal_relu(MTLDeviceRef device, MTLCommandQueueRef queue,
               const float* input, float* output, int size) {
    @autoreleasepool {
        id<MTLDevice> mtlDevice = (__bridge id<MTLDevice>)device;
        id<MTLCommandQueue> commandQueue = (__bridge id<MTLCommandQueue>)queue;
        
        // Use Metal Performance Shaders ReLU
        MPSCNNNeuronReLU* relu = [[MPSCNNNeuronReLU alloc] initWithDevice:mtlDevice];
        
        // Create 1D image descriptor (treat as 1D array)
        MPSImageDescriptor* imageDesc = [MPSImageDescriptor imageDescriptorWithChannelFormat:MPSImageFeatureChannelFormatFloat32
                                                                                        width:size
                                                                                       height:1
                                                                                 featureChannels:1];
        
        id<MTLBuffer> inputBuffer = [mtlDevice newBufferWithBytes:input
                                                           length:size * sizeof(float)
                                                          options:MTLResourceStorageModeShared];
        
        id<MTLBuffer> outputBuffer = [mtlDevice newBufferWithLength:size * sizeof(float)
                                                            options:MTLResourceStorageModeShared];
        
        MPSImage* inputImage = [[MPSImage alloc] initWithBuffer:inputBuffer
                                                     descriptor:imageDesc];
        MPSImage* outputImage = [[MPSImage alloc] initWithBuffer:outputBuffer
                                                      descriptor:imageDesc];
        
        id<MTLCommandBuffer> commandBuffer = [commandQueue commandBuffer];
        [relu encodeToCommandBuffer:commandBuffer sourceImage:inputImage destinationImage:outputImage];
        [commandBuffer commit];
        [commandBuffer waitUntilCompleted];
        
        memcpy(output, [outputBuffer contents], size * sizeof(float));
        
        return 0; // Success
    }
}


